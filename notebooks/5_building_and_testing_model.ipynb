{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8435b32c-dee6-4d09-91d8-b325b2f4d693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefc0c22-489a-4e8b-9946-7ae2f90192a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2016 = pd.read_csv('https://drive.google.com/uc?id=15GlcdLJ79bc5_WhVNViepQaXvsE1vYb8')\n",
    "properties_2016 = pd.read_csv('/Users/yang/Downloads/properties_2016.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf3b701-1e39-4c38-99ef-0cebe6bdfef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "training_data = pd.merge(train_2016, properties_2016, on=['parcelid'], how='inner')\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14d7533-1160-4183-8e03-a0bca07902c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfomers\n",
    "class BinaryNullTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.assign(**{col: X[col].notnull() for col in self.columns})\n",
    "    \n",
    "    \n",
    "class IntervalCategorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column, rng=(2,4)):\n",
    "        self.column = column\n",
    "        self.rng = rng\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.assign(**{self.column: np.where((X[self.column] >= self.rng[0]) & (X[self.column] <= self.rng[1]), True, False)})\n",
    "    \n",
    "    \n",
    "class Normalizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.means = X[self.columns].mean()\n",
    "        self.std = X[self.columns].std()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.assign(**{col: (X[col] - self.means[col]) / self.std[col] for col in self.columns})\n",
    "    \n",
    "\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.columns]\n",
    "    \n",
    "# New transformer to just do simple imput on each column\n",
    "class DataframeImputer(SimpleImputer):\n",
    "    def transform(self, X):\n",
    "        return pd.DataFrame(super().transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa65ee1-0708-4b54-bbff-604f70fd91b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "exist_or_not_variables = ['garagecarcnt', 'yardbuildingsqft26', 'basementsqft', 'fireplacecnt', 'yardbuildingsqft17']\n",
    "bedrooms = ['bedroomcnt']\n",
    "bathrooms = ['fullbathcnt']\n",
    "normalized_variables = ['finishedsquarefeet12', 'structuretaxvaluedollarcnt']\n",
    "other_variables = ['yearbuilt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1a1ad1-d867-42ea-be0b-a3dd7af66e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('binary_null', BinaryNullTransformer(exist_or_not_variables)),\n",
    "        ('bedrooms', IntervalCategorizer(*bedrooms, (2, 4))),\n",
    "        ('bathrooms', IntervalCategorizer(*bathrooms, (2, 4))),\n",
    "        ('normalize', Normalizer(normalized_variables)),\n",
    "        ('select_features', FeatureSelector([*exist_or_not_variables, *bedrooms, *bathrooms, *normalized_variables, *other_variables])),\n",
    "        ('impute_nulls', DataframeImputer(missing_values=np.nan, strategy='mean'))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9d8799-81af-482c-b6b1-2de0c14331c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a tree, but use our previous pipeline.  Note that a pipeline can be part of a pipeline as well\n",
    "model_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('transformer', transformer_pipeline),\n",
    "        ('model', RandomForestRegressor(n_estimators=100, random_state=10))\n",
    "    ]   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8c64f2-8a0a-4fa4-b12e-4218a412d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model on training data\n",
    "fitted_model = model_pipeline.fit(training_data, training_data['logerror'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363decfc-dc5e-46d9-9983-53dfdcc6c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's test that the model works\n",
    "(lambda x: x.assign(prediction=fitted_model.predict(x)))(training_data.sample(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9485f4c-af24-4830-8620-e434c59450a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great - but that is in sample.  We want to validate the model out of sample, so how do we know that it's the best we can do?  We'll need to cross-validate\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "for train_idx, test_idx in kf.split(training_data):\n",
    "    X_train, X_test = training_data.iloc[train_idx], training_data.iloc[test_idx]\n",
    "\n",
    "    fitted_model = model_pipeline.fit(X_train, X_train['logerror'])\n",
    " \n",
    "    y_pred = fitted_model.predict(X_test)\n",
    "    metric = metrics.mean_absolute_error(X_test['logerror'], y_pred)\n",
    "    print(metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fb5a42-c060-4c3c-af65-d2a33338b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now know our general performance.  However, how do we know that the hyperparameters of the model are tuned correctly?\n",
    "# We can use CV to tune them\n",
    "param_grid = {\n",
    "    'model__bootstrap': [True],\n",
    "    'model__max_depth': [80, 110],\n",
    "    'model__max_features': [2, 3],\n",
    "    'model__min_samples_leaf': [3, 5],\n",
    "    'model__min_samples_split': [8, 12],\n",
    "    'model__n_estimators': [100, 300]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model_pipeline, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "grid_search.fit(training_data, training_data['logerror'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3d205f-9a1f-464e-ba66-3ef565d3ea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's evaluate the best model from hyperparameter tuning\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "metrics.mean_absolute_error(training_data['logerror'], best_model.predict(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe150570-2684-44dd-9484-5f37ebb649bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great, we have found our model.  Let's save it so it can be used later\n",
    "pickle.dump(best_model, open('/Users/yang/Downloads/mfe_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452efde2-35b0-4551-8a3e-4b95f6a4ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's verify that the saved model works\n",
    "loaded_model = pickle.load(open('/Users/yang/Downloads/mfe_model.pkl', 'rb'))\n",
    "\n",
    "metrics.mean_absolute_error(training_data['logerror'], best_model.predict(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e03039-080a-476e-997d-0fba4fe4d49b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
